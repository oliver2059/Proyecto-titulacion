import cv2
import numpy as np

def non_max_suppression(boxes, confidences, threshold):
    idxs = cv2.dnn.NMSBoxes(boxes, confidences, threshold, threshold)
    if len(idxs) > 0:
        return idxs.flatten()
    else:
        return []
cv2: Biblioteca OpenCV para procesamiento de imágenes y video.
numpy: Biblioteca para cálculos numéricos.
non_max_suppression: Esta función elimina detecciones redundantes.
Configuración del Modelo
Cargamos la configuración, los pesos del modelo YOLOv3 y las etiquetas de las clases.

python
Copiar código
config = "model/yolov3.cfg"
weights = "model/yolov3.weights"
LABELS = open("model/coco.names").read().strip().split("\n")
colors = np.random.randint(0, 255, size=(len(LABELS), 3), dtype="uint8")
net = cv2.dnn.readNetFromDarknet(config, weights)
config: Archivo con la configuración del modelo.
weights: Archivo con los pesos entrenados del modelo.
LABELS: Lista de nombres de las clases que el modelo puede detectar.
colors: Colores aleatorios para dibujar las cajas alrededor de los objetos.
net: Carga el modelo YOLOv3 en OpenCV.
Captura de Video
Inicializamos la captura de video desde la cámara.

python
Copiar código
cap = cv2.VideoCapture(1)  # Cambia el índice si es necesario

if not cap.isOpened():
    print("Error: No se puede acceder a la cámara.")
else:
    print("Cámara iniciada.")
cap: Objeto que captura el video de la cámara.
Verificamos si la cámara se abrió correctamente.
Procesamiento del Video
Procesamos cada frame del video en un bucle.

python
Copiar código
while True:
    ret, frame = cap.read()

    if not ret:
        print("Error: No se puede recibir frame (stream end?). Saliendo ...")
        break

    height, width = frame.shape[:2]
 blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)
    net.setInput(blob)

    ln = net.getLayerNames()
    ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]

    outputs = net.forward(ln)

    boxes = []
    confidences = []
    classIDs = []

    for output in outputs:
        for detection in output:
            scores = detection[5:]
            classID = np.argmax(scores)
            confidence = scores[classID]

            if confidence > 0.5:
                box = detection[:4] * np.array([width, height, width, height])
                (x_center, y_center, w, h) = box.astype("int")
                x = int(x_center - (w / 2))
                y = int(y_center - (h / 2))

                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                classIDs.append(classID)

    idxs = non_max_suppression(boxes, confidences, 0.5)

    if len(idxs) > 0:
        for i in idxs:
 (x, y) = (boxes[i][0], boxes[i][1])
            (w, h) = (boxes[i][2], boxes[i][3])

            color = colors[classIDs[i]].tolist()
            text = "{}: {:.3f}".format(LABELS[classIDs[i]], confidences[i])
            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
            cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    cv2.imshow('frame', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
Bucle principal: Captura y procesa cada frame de la cámara.
Captura de frames: ret, frame = cap.read()
